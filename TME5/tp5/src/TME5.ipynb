{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAL TP5\n",
    "## Réseaux récurrents LSTM, GRU, autres cellules de mémoire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment traiter et générer des séquences de taille variable?*\n",
    "\n",
    "*Comment prendre en compte des dépendances à plus long terme (Vanishing, exploding, gradients)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Générations de séquences de taille variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous considérons que chaque séquence est une phrase (qui peut se terminer par un point, un point d'interrogation ou un point d'exclamation).\n",
    "\n",
    "Dès lors il faut:\n",
    "- employer un symbole spécial qui marque la fin d'une séquence EOS. Lors de l'apprentissage, il faut ajouter ce token à chaque séquence pour apprendre à le prédire.\n",
    "- padder chaque séquence (ajouter un caractère nul (BLANK) autant de fois qu'il est nécessaire afin que les séquences d'un même batch aient toutes la même longueur, ce caractère devra être ignoré lors de l'apprentissage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1.\n",
    "Dans `testloader.py` la classe `TextDataset` est telle que:\n",
    "- chaque exemple est une phrase\n",
    "- la taille du dataset est le nbr de phrases ds le corpus\n",
    "- la phrase renvoyée est sous forme d'une séquence d'entiers, chaque entier codant pr un caractère (application de la fonciton `string2code`)\n",
    "Cette classe renvoie des exemples de longueur variable. Il faut préciser au DataLoader de quelle manière regrouper les exemples pr construire un batch. C'est le rôle de l'argument `collate_fn` du constructeur d'un DataLoader qui prend une fonction en paramètre.\n",
    "Définir une fonction `pad_collate_fn` qui prépare un tenseur batch (de taille `longueur x taille du batch` où la longueur = longeur max de la séquence du batch) à partir d'une liste d'exemples du `TextDataset`: elle doit ajouter le code du symbole EOS à la fin de chaque exemple et padder les séquences avec le code du caractère nul. Exécuter `textloader.py`pr vérifier que tout foncitonne bien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De plus, il faut prendre soin de ne pas inclure le padding lorsqu’on calcule le coût ici le maximum de vraisemblance : pour cela, la solution la plus courante est d’utiliser un masque binaire (0 lorsque le caractère est nul, 1 sinon) qui est  multiplié avec les log-probabilités avant\n",
    "de les additionner (le paramètre `reduce=\"none\"` dans une fonction de coût\n",
    "(en particulier, pour la cross entropie) permet d’obtenir le coût pour chaque élément plutôt que la moyenne).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chaîne à code :  C'est. Un. Test.\n",
      "Shape ok\n",
      "encodage OK\n",
      "Token EOS ok\n",
      "Token BLANK ok\n",
      "Chaîne décodée :  C'est. Un. Test.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nolwenn/AMAL/TME5/tp5/src/textloader.py:72: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  samples = [torch.cat((torch.tensor(seq), torch.tensor([EOS_IX]))) for seq in samples]\n"
     ]
    }
   ],
   "source": [
    "%run textloader.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2.\n",
    "\n",
    "Créer votre fonction de coût `maskedCrossEntropy(output, target, padcar) ` dans `tp5.py` qui permet de calculer le coût sans prendre en compte les caracètres nuls en\n",
    "fonction de la sortie obtenue output, la sortie désirée target et le code de caractère\n",
    "de padding padcar. Vous ferez attention à n’utiliser aucune boucle pour ce calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tp5 import maskedCrossEntropy, RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1889891624450684\n"
     ]
    }
   ],
   "source": [
    "# Dummy data\n",
    "output_dim = 10\n",
    "length = 5\n",
    "batch_size = 3\n",
    "padcar = 0\n",
    "\n",
    "# Randomly generated tensors for output and target\n",
    "output = torch.rand(length, batch_size, output_dim)\n",
    "target = torch.randint(0, output_dim, (length, batch_size), dtype=torch.long)\n",
    "\n",
    "# Call maskedCrossEntropy\n",
    "#lossCecile = maskedCEcile(output, target, padcar)\n",
    "loss = maskedCrossEntropy(output, target, padcar)\n",
    "\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. \n",
    "Dans `generate.py` implémenter une fonction de génération de façon à générer des séquences jusqu'à rencontrer le caractère EOS (penser à prévoir une taille maximum tout de même). Prévoir de faire de la génération aléatoire dans la distribution obtenue ou déterministe en choisissant le caractère le plus probable à chaque pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez toutes les classes nécessaires\n",
    "from torch import nn\n",
    "import math\n",
    "from textloader import string2code, code2string, id2lettre, lettre2id\n",
    "from tp5 import RNN  # Remplacez par le nom réel de votre module\n",
    "from generate import *\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Créez une instance de votre modèle RNN\n",
    "input_dim = 10  # Remplacez par la dimension appropriée\n",
    "latent_dim = 64  # Remplacez par la dimension appropriée\n",
    "output_dim = 10  # Remplacez par la dimension appropriée\n",
    "\n",
    "rnn_model = RNN(input_dim, latent_dim, output_dim)\n",
    "\n",
    "# Example: Create dummy embedding and decoder layers\n",
    "embed_layer = nn.Embedding(num_embeddings = len(lettre2id), embedding_dim = 50, padding_idx = lettre2id['<PAD>']).to(device)\n",
    "latent_dim = len(lettre2id)\n",
    "output_dim = len(lettre2id) + 1\n",
    "decoder_layer = RNN.decode\n",
    "\n",
    "# Paramètres de la génération\n",
    "eos_token = 0  # Remplacez par le code de votre caractère EOS\n",
    "\n",
    "start_sequence = \"C'est un test.\"  # Remplacez par le texte de départ souhaité\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x50 and 10x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test generate function\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Beam Sequence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_sequence)\n",
      "File \u001b[0;32m~/AMAL/TME5/tp5/src/generate.py:35\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(rnn, emb, decoder, eos, start, maxlen, C)\u001b[0m\n\u001b[1;32m     33\u001b[0m     ht \u001b[38;5;241m=\u001b[39m rnn\u001b[38;5;241m.\u001b[39mforward(x_emb, h, Ct)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     ht \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Calcul du premier élément de la séquence\u001b[39;00m\n\u001b[1;32m     38\u001b[0m distribution \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLogSoftmax(decoder(ht) )\n",
      "File \u001b[0;32m~/AMAL/TME5/tp5/src/tp5.py:74\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Appel de la méthode one_step sur nos séquences à chaque instant i\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)):\n\u001b[0;32m---> 74\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     hidden_states\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(hidden_states)\n",
      "File \u001b[0;32m~/AMAL/TME5/tp5/src/tp5.py:60\u001b[0m, in \u001b[0;36mRNN.one_step\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Traite un pas de temps: renvoie le prochain état caché.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m        @param x: torch.Tensor, batch des séquences à l'instant t de taille (batch, input)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m        @param h: torch.Tensor, batch des états cachés à l'instant t-1 de taille (batch, latent)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtanh( \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_lat(h) )\n",
      "File \u001b[0;32m~/anaconda3/envs/deepdac/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepdac/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/deepdac/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x50 and 10x64)"
     ]
    }
   ],
   "source": [
    "# Test generate function\n",
    "\n",
    "generated_sequence = generate(rnn_model, embed_layer, decoder_layer, eos_token, start_sequence)\n",
    "\n",
    "print(\"Generated Beam Sequence:\", generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test generate_beam function\u001b[39;00m\n\u001b[1;32m      3\u001b[0m beam_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Replace with your desired beam size\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m generated_beam_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_beam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Beam Sequence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_beam_sequence)\n",
      "File \u001b[0;32m~/AMAL/TME5/tp5/src/generate.py:78\u001b[0m, in \u001b[0;36mgenerate_beam\u001b[0;34m(rnn, emb, decoder, eos, k, start, maxlen)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m gen_len \u001b[38;5;241m<\u001b[39m maxlen:\n\u001b[1;32m     76\u001b[0m     new_sequences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequence, score \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# Récupérer le dernier élément de la séquence\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         last_element \u001b[38;5;241m=\u001b[39m sequence[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;66;03m# Embedding de la séquence\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Test generate_beam function\n",
    "\n",
    "beam_k = 3  # Replace with your desired beam size\n",
    "\n",
    "generated_beam_sequence = generate_beam(rnn_model, embed_layer, decoder_layer, eos_token, beam_k, start=start_sequence)\n",
    "\n",
    "print(\"Generated Beam Sequence:\", generated_beam_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decode() missing 1 required positional argument: 'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Example state tensor (replace with your actual state tensor)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m example_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, latent_dim)\n\u001b[0;32m----> 7\u001b[0m nucleus_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnucleus_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNucleus Sampling Probabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, nucleus_probs)\n",
      "File \u001b[0;32m~/AMAL/TME5/tp5/src/generate.py:133\u001b[0m, in \u001b[0;36mp_nucleus.<locals>.compute\u001b[0;34m(h)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calcule la distribution de probabilité sur les sorties\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m   * h (torch.Tensor): L'état à décoder\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#  TODO:  Implémentez le Nucleus sampling ici (pour un état s)\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Forward sur le modèle RNN et calcul des logits\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Application du softmax pour obtenir les probabilités\u001b[39;00m\n\u001b[1;32m    136\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: decode() missing 1 required positional argument: 'h'"
     ]
    }
   ],
   "source": [
    "alpha_value = 0.7  # Replace with your desired alpha value\n",
    "nucleus_function = p_nucleus(decoder_layer, alpha_value)\n",
    "\n",
    "# Example state tensor (replace with your actual state tensor)\n",
    "example_state = torch.randn(1, latent_dim)\n",
    "\n",
    "nucleus_probs = nucleus_function(example_state)\n",
    "print(\"Nucleus Sampling Probabilities:\", nucleus_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de TP4, on a utilisé un encodage onehot pr chaque caractère, suivi d'un module linéaire. Le module `nn.Embedding` de `Torch` permet de combiner ces 2 étapes pr éviter la création (non nécessaire et coûteuse) de vecteurs one hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prise en compte de dépendences lointaines: LSTM et GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
