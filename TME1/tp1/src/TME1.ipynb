{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMAL - TP1\n",
    "## Définition de fonctions en pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Fonction dans un graphe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise la règle de la dérivée en chaîne.\n",
    "\n",
    "$$\\frac{\\partial (L \\circ h)}{\\partial x_i}(x) = \\sum_{j=1}^{p} \\frac{\\partial(L \\circ h)}{\\partial\\ y_j} \\cdot \\frac{\\partial h_j}{\\partial x_i}(x)=\\sum_{j=1}^{p} (\\nabla L)_j \\cdot \\frac{\\partial h_j}{\\partial x_i}(x)$$\n",
    "\n",
    "La dérivée de la composition $L\\circ h$ par rapport à $x_i$ dépend des dérivées partielles de $h_j$ par rapport à $x_i$ et des dérivées de la fonctions de coût $L$ par rapport à ses sorties $y_j$. \n",
    "\n",
    "Pour calculer le gradient d'une fonction composée, il suffit de connaître le gradient de chaque fonction par rapport à chacune de ses entrées et appliquer une dérivation chaînée. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le contexte des réseaux de neurones, chaque couche du réseau est essentiellement une fonction composée de transformations linéaires (pondérées par les poids des connexions) et de fonctions d'activation non linéaires. \n",
    "\n",
    "Lors de l'apprentissage, on calcule les gradients des poids par rapport à la fonction de coût, ce qui implique d'appliquer la règle de la chaîne pour obtenir ces gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward** Calcul de $y = h(x)$ en fonction de $x$. Les donénes d'entrées sont introduites dans le réseau de neurones puis passent par chaque couche du réseau, où des transformations linéaires et des fonctions d'activation non linéaires sont appliquées. Génère les prédictions (sorties) du réseau\n",
    "\n",
    "**Backward** Calcul de $\\frac{h}{x}(x)$ et retour en prenant en compte les $\\nabla L(y)$ passés en paramètres et de données dérivées de la phase forward $(x)$. Les gradients de la perte par rapport aux paramètres du modèles sont calculés. Calcul itératif, commençant par la dernière couche du modèle (sortie) et se propageant ensuite en sens inverse à travers les couches du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Application : Régression Linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Calcul du gradient (scalaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de régression linéaire peut être vu comme la composition de 2 éléments:\n",
    "- une fonction linéaire, responsable du calcul de la prédiction: $$\\hat{y} = f(x,w,b) = x \\cdot w + b$$\n",
    "- un coût aux moindres carrés (MSE): $$mse(\\hat{y}, y)=(\\hat{y}-y)^2$$\n",
    "\n",
    "Afin d'effectuer une descente de gradient, il faut calculer le gradient du coût par rapport aux paramètres w et b. Nous allons calculer ce gradient grâce au chaînage des dérivées partielles. \n",
    "\n",
    "Nous supposons que le coût est défini de la manière suivante: $$C(x,w,b,y) = mse(f(x,w,b),y)$$ où $x$ est l'entrée, $y$ la sortie désirée dans $\\mathbb{R}$ et $w \\in \\mathbb{R}^n$ et $b \\in \\mathbb{R}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction Loss MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la fonction de coût MSE, les dérivées par rapport à $\\hat{y}$ et $y$ sont les mêmes.\n",
    "\n",
    "Calcul de $\\frac{\\partial(L \\circ mse)}{\\partial y}(\\hat{y}, y)$ :\n",
    "\n",
    "En utilisant la règle de la chaîne, nous avons $$\\frac{\\partial(L \\circ mse)}{\\partial y} = \\frac{\\partial mse(\\hat{y}, y)}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial y}$$\n",
    "\n",
    "$$\\frac{\\partial mse(\\hat{y}, y)}{\\partial \\hat{y}} = 2(\\hat{y} - y)$$\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}}{\\partial y} = 1$$\n",
    "\n",
    "Donc, $$\\frac{\\partial(L \\circ mse)}{\\partial y} = 2(\\hat{y} - y)$$ et $$\\frac{\\partial(L \\circ mse)}{\\partial \\hat{y}} = 2(\\hat{y} - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction Linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer $$\\frac{\\partial(L \\circ f)}{\\partial X}(x, w, b) =  \\frac{\\partial(L \\circ f)}{\\partial \\hat{y}}\\cdot \\frac{\\partial \\hat{y}}{\\partial X}$$\n",
    "La première partie $\\frac{\\partial(L \\circ f)}{\\partial \\hat{y}}$ est la dérivée de la perte MSE par rapport à la sortie de la couche linéaire.\n",
    "\n",
    "La deuxième partie $\\frac{\\partial \\hat{y}}{\\partial X}$ est la dérivée de la sortie de la couche linéaire par rapport à X. \n",
    "\n",
    "D'où $$\\frac{\\partial(L \\circ f)}{\\partial X}(x, w, b) =  (\\hat{y} - y) \\cdot W$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pour calculer $$\\frac{\\partial(L \\circ f)}{\\partial W}(X, W, b) =  \\frac{\\partial(L \\circ f)}{\\partial \\hat{y}}\\cdot \\frac{\\partial \\hat{y}}{\\partial W}$$\n",
    " D'où $$\\frac{\\partial(L \\circ f)}{\\partial W}(X, W, b) =  (\\hat{y} - y) \\cdot X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Pour calculer $$\\frac{\\partial(L \\circ f)}{\\partial b}(X, W, b) =  \\frac{\\partial(L \\circ f)}{\\partial \\hat{y}}\\cdot \\frac{\\partial \\hat{y}}{\\partial b}$$\n",
    " D'où $$\\frac{\\partial(L \\circ f)}{\\partial b}(X, W, b) =  (\\hat{y} - y) \\cdot 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Calcul du gradient (matriciel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On considère maintenant un cas plus général où nous travaillons avec un lot( batch) de *q* exemples (au lieu d'un) et *p* sorties (au lieu d'une).\n",
    "- Nous avons toujours une fonction linéaire, responsable du calcul de la prédiction: $$ \\hat{Y} = f(X, W, b) = XW + \\begin{bmatrix}\n",
    "b \\\\\n",
    "\\vdots \\\\\n",
    "b\n",
    "\\end{bmatrix} $$\n",
    "où $X \\in \\mathbb{R}^{q \\times n}$ sont les entrées, $W \\in \\mathbb{R}^{n \\times p}$ est la matrice de poids, et $b \\in \\mathbb{R}^{1 \\times p}$ est le biais. La sortie $Y$ est donc une matrice dans $\\mathbb{R}^{q \\times p}\n",
    "- un coût aux moindres carrés (MSE) que l'on généralise a un lot d'exemples et plusieurs sorties : $mse(\\hat{Y}, Y) = \\frac{1}{q}||\\hat{Y} - Y||^2$. Notez que l'on divise par le nombre d'exemple, ceq ui permet d'avoir une magnitude du gradient qui ne vaie pas en fonction du nombre d'exemples. \n",
    "\n",
    "Le coût $C$ est alors défini comme $$ C(X, W, b, Y) = mse(f(X, W, b), Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction Loss MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer $$\\frac{\\partial L \\circ mse}{\\partial Y_{ij}}(\\hat{Y}, Y) = \\frac{\\partial L \\circ mse}{\\partial \\hat{Y}_{ij}}(\\hat{Y}, Y)$$ \n",
    "\n",
    "Avec la méthode de dérivation en chaîne, obtient $$\\frac{2}{q} \\sum_{j=1}^{q}(\\hat{Y} - Y)$$\n",
    "\n",
    "NB : Pour la loss MSE les dérivées partielles sont égales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fonction Linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer $$\\frac{\\partial(L \\circ mse)}{\\partial X_{ij}}(X, W, b) =  \\frac{2}{q}(\\hat{Y_{ij}} - Y_{ij}) \\cdot \\frac{\\partial f_{ij}}{\\partial X_{ij}}(X, W, b)$$\n",
    "D'où $$\\frac{\\partial(L \\circ mse)}{\\partial X_{ij}}(X, W, b) =  \\frac{2}{q}(\\hat{Y_{ij}} - Y_{ij}) \\cdot W_{ij}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer $$\\frac{\\partial(L \\circ mse)}{\\partial W_{ij}}(X, W, b) =  \\frac{2}{q}(\\hat{Y_{ij}} - Y_{ij}) \\cdot \\frac{\\partial f_{ij}}{\\partial W_{ij}}(X, W, b)$$\n",
    "D'où $$\\frac{\\partial(L \\circ mse)}{\\partial W_{ij}}(X, W, b) =  \\frac{2}{q}(\\hat{Y_{ij}} - Y_{ij}) \\cdot X_{ij}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour calculer $$\\frac{\\partial(L \\circ mse)}{\\partial b_j}(X, W, b) =  \\frac{2}{q}(\\hat{Y_{ij}} - Y_{ij}) \\cdot \\frac{\\partial f_{ij}}{\\partial W_{ij}}(X, W, b_j)$$\n",
    "D'où $$\\frac{\\partial(L \\circ mse)}{\\partial b{j}}(X, W, b) = \\frac{2}{q}\\sum_{j=1}^{q}(\\hat{Y} - Y)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forme matricielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposons que :\n",
    "- $X$ est une matrice de forme (p, q) où p est le nombre de caractéristiques d'entrée et q est le nombre d'exemples.\n",
    "- $W$ est une matrice de forme (p, r) où r est le nombre de neurones dans la couche de sortie.\n",
    "- $b$ est un vecteur de forme (r, 1) où r est le nombre de neurones dans la couche de sortie.\n",
    "- $\\hat{Y}$ est la prédiction du modèle de forme (r, q).\n",
    "$Y$ est la vérité de terrain de forme (r, q)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L \\circ mse}{\\partial Y_{ij}}(\\hat{Y}, Y) = \\frac{\\partial L \\circ mse}{\\partial \\hat{Y}_{ij}}(\\hat{Y}, Y) = \\frac{2}{q}(\\hat{Y} - Y)$$\n",
    "\n",
    "$$\\frac{\\partial(L \\circ mse)}{\\partial X} = \\frac{2}{q} (\\hat{Y} - Y) \\cdot W^T$$\n",
    "\n",
    "$$\\frac{\\partial(L \\circ mse)}{\\partial W} = \\frac{2}{q} X \\cdot (\\hat{Y} - Y)^T $$\n",
    "\n",
    "$$\\frac{\\partial(L \\circ mse)}{\\partial b} =  \\frac{2}{q}(\\hat{Y_{ij}} - Y_{ij}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Calcul du gradient de C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons calculé toutes les dérivées partielles, il faut montrer comment la dérivée de C par rapport à chacune de ses entrées.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dérivée par rapport à W: $$\\frac{\\partial C}{\\partial W} = \\frac{\\partial mse}{\\partial \\hat{Y_ij}} \\cdot \\frac{\\partial f_{ij}}{\\partial W_{ij}} = \\frac{2}{q}(\\hat{Y_{ij}} - Y_{ij}) \\cdot X_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dérivée par rapport à b : $$\\frac{\\partial C}{\\partial b} = \\frac{\\partial mse}{\\partial \\hat{Y_{ij}}} \\cdot \\frac{\\partial f_{ij}}{\\partial b_j} = \\frac{2}{q} \\sum_{i=1}^{q} (\\hat{Y}_i - Y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les dérivées de la fonction coût sont essentielles pour la mise à jour des poids et du biais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
